<div align="center">

## ğŸ¹ [ACMMM '25] Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated Piano Hand Motion Synthesis

ğŸ  [Homepage](https://monkek123King.github.io/S2C_page) Â Â Â Â  ğŸ“„ [Paper](https://arxiv.org/abs/2504.09885) Â Â Â Â  ğŸ’¾ Dataset [[Google Drive](https://drive.google.com/drive/folders/1JY0zOE0s7v9ZYLlIP1kCZUdNrih5nYEt?usp=sharing)]/[[Hyper.ai](https://hyper.ai/datasets/32494)]/[[Zenodo](https://zenodo.org/records/13297386)] Â Â Â Â  ğŸ¤— Model [[HuggingFace](https://huggingface.co/thuteam/S2C/tree/main)]

</div>

-----

### ğŸ“¢ News

  * **`Sept 2025`:** Experiment checkpoints are released [here](https://huggingface.co/thuteam/S2C)\! ğŸ‰
  * **`July 2025`:** Our paper has been accepted to ACMMM 2025\! ğŸ¥³
  * **`April 2025`:** The paper is now available on [arXiv](https://arxiv.org/abs/2504.09885). â˜•ï¸

-----

## ğŸš€ Getting Started

### ğŸ”§ Installation

**a. Create a conda virtual environment and activate it.**
```shell
conda create -n S2C python=3.10 -y
conda activate S2C
```

**b. Install PyTorch and torchvision following the [official instructions](https://pytorch.org/).**
```shell
pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118
```

**c. Clone S2C.**
```
git clone https://github.com/monkek123King/S2C.git
```

**d. Install other requirements.**
```shell
cd S2C
pip install -r requirement.txt
```

**e. Prepare MANO models.**

Besides, you also need to download the MANO model. Please visit the [MANO website](https://mano.is.tue.mpg.de/) and register to get access to the downloads section. We only require the right hand model. You need to put MANO_RIGHT.pkl under the ./mano folder.

**f. Prepare pretrained models. (Used in training.)**

Download pretrained HuBert([Large](https://huggingface.co/facebook/hubert-large-ls960-ft))  to `S2C/checkpoints`.

**g. Prepare Gesture Autoencoder model. (Used in evaluation.)**

Download pretrained [Gesture Autoencoder model](https://drive.google.com/file/d/1G2Fe_zlJn8I_U_VGldH4SsIa_KauvG3p/view?usp=sharing) to `S2C/checkpoints`

```
checkpoints
â”œâ”€â”€ gesture_autoencoder_checkpoint_best.bin
â”œâ”€â”€ hubert-large-ls960-ft/
```

### ğŸ“¦ Prepare Dataset

**PianoMotion10M**

Download PianoMotion10M V1.0 full dataset data [HERE](https://drive.google.com/drive/folders/1JY0zOE0s7v9ZYLlIP1kCZUdNrih5nYEt?usp=sharing).

```
cd /path/to/PianoMotion10M_Dataset
unzip annotation.zip
unzip audio.zip
unzip midi.zip
```


**Folder structure**
```
/path/to/PianoMotion10M_Dataset
â”œâ”€â”€ annotation/
â”‚   â”œâ”€â”€ 1033685137/
â”‚   â”‚   â”œâ”€â”€ BV1f34y1i7U1/
â”‚   â”‚   â”‚   â”œâ”€â”€BV1f34y1i7U1_seq_0000.json
â”‚   â”‚   â”‚   â”œâ”€â”€BV1f34y1i7U1_seq_0001.json
â”‚   â”‚   â”‚   â”œâ”€â”€...
â”‚   â”‚   â”œâ”€â”€ BV1X44y1J7CR/
â”‚   â”œâ”€â”€ 2084102325/
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ 1033685137/
â”‚   â”‚   â”œâ”€â”€ BV1f34y1i7U1/
â”‚   â”‚   â”‚   â”œâ”€â”€BV1f34y1i7U1_seq_0000.mp3
â”‚   â”‚   â”‚   â”œâ”€â”€BV1f34y1i7U1_seq_0001.mp3
â”‚   â”‚   â”‚   â”œâ”€â”€...
â”‚   â”‚   â”œâ”€â”€ BV1X44y1J7CR/
â”‚   â”œâ”€â”€ 2084102325/
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ midi/
â”‚   â”œâ”€â”€ 1033685137/
â”‚   â”‚   â”œâ”€â”€ BV1f34y1i7U1.mid
â”‚   â”‚   â”œâ”€â”€ BV1X44y1J7CR.mid
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ 2084102325/
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ train.txt
â”œâ”€â”€ test.txt
â”œâ”€â”€ valid.txt
```

**Usage**

`draw.py` shows the usage of our dataset and visualizes some samples of hand motions under `./draw_sample`.

```shell
python draw.py
```

### ğŸ‹ï¸ Train and Evaluate

**Please ensure you have prepared the environment and the PianoMotion10M dataset.**

**Train and Test**

Train S2C Position Predictor with Hubert and transformer. Feel free to change audio feature extractor by `--wav2vec_path`. The result will be stored in `./logs/`.
```
python train.py --experiment_name piano2posi_LR --bs_dim 6 --adjust --is_random --up_list 1467634 66685747 \
--data_root ./ --iterations 200000 --batch_size 8 --train_sec 8 --feature_dim 512 \
--wav2vec_path ./checkpoints/hubert-large-ls960-ft --check_val_every_n_iteration 1000 --save_every_n_iteration 1000 \
--latest_layer tanh --encoder_type transformer --num_layer 4
```

Train S2C Gesture Generator with Hubert and transformer. The result will be stored in `./logs/`.
```
python train_diffusion.py --experiment_name piano2mot --is_random --unet_dim 256 --iterations 800000 \
--bs_dim 96  --batch_size 16 --train_sec 8 --data_root ./ \
--xyz_guide  --check_val_every_n_iteration 1000 --save_every_n_iteration 1000 \
--adjust --piano2posi_path logs/piano2posi_LR --encoder_type transformer --num_layer 4 \
--lr 1e-5 --fusion 4 --obj pred_v
```

Eval S2C after training S2C Gesture Generator on the validation set.
```
python eval.py --exp_path /path/to/logs (e.g. ./logs/piano2mot)  --data_root /path/to/PianoMotion10M_Dataset --valid_batch_size 64 --mode valid
```

**Visualization**

Visualize the results, which will be stored in `./results`.

```
python infer.py --exp_path /path/to/logs --data_root /path/to/PianoMotion10M_Dataset --valid_batch_size 64 --mode valid
```

-----

## âœï¸ Citation

If you find our work useful for your research, please consider citing our paper and giving this repository a star ğŸŒŸ.

```bibtex
@inproceedings{liu2025separate,
  title={Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated Piano Hand Motion Synthesis},
  author={Liu, Zihao and Ou, Mingwen and Xu, Zunnan and Huang, Jiaqi and Han, Haonan and Li, Ronghui and Li, Xiu},
  booktitle={Proceedings of the 33rd ACM International Conference on Multimedia},
  pages={9743--9752},
  year={2025}
}
```
